# -*- coding: utf-8 -*-
"""
Created on Wed Jul 13 20:50:57 2022

@author: lutzbueno_v

This function automatically marges the data collected in different detector distances
"""

import numpy as np
import matplotlib.pyplot as plt
import pickle
import integration as integ
from scipy.interpolate import interp1d
import scipy.optimize
import os
from utils import smooth
from scipy.stats import linregress # To check for linearity/constancy

# %% plot_all_data

def plot_all_data(path_dir_an):
    """
    Loads radial integration data from different detector distances, organizes it
    by sample name, and plots the raw (unmerged) data for visual inspection.

    Args:
        path_dir_an (str): The path to the main analysis directory where results
                           and configuration files are stored, and where detector-specific
                           integration data is located.

    Returns:
        dict: A dictionary where keys are sample names and values are dictionaries
              containing 'I' (intensities), 'q' (scattering vectors), and 'error'
              (standard deviations) from all relevant detector distances, stacked
              for each sample.
    """
    # Create necessary subdirectories for merged data and figures if they don't exist
    path_merged = os.path.join(path_dir_an, 'merged/')
    if not os.path.exists(path_merged):
        os.mkdir(path_merged)
    path_merged_fig = os.path.join(path_merged, 'figures/')
    if not os.path.exists(path_merged_fig):
        os.mkdir(path_merged_fig)
    path_merged_txt = os.path.join(path_merged, 'data_txt/')
    if not os.path.exists(path_merged_txt):
        os.mkdir(path_merged_txt)

    # Load result and configuration files
    file_results = os.path.join(path_dir_an, 'result.npy')
    with open(file_results, 'rb') as handle:
        result = pickle.load(handle)
    file_config = os.path.join(path_dir_an, 'config.npy')
    with open(file_config, 'rb') as handle:
        config = pickle.load(handle)

    calibration = config['experiment']['calibration']
    list_class_files = result['overview']

    merged_files = {} # Dictionary to store intensities, q, and errors for each sample

    # Iterate through all detector distances found in the result overview
    for keys in list_class_files:
        if 'det' in keys: # Filter for detector-specific keys
            total_samples = len(list_class_files[keys]['scan'])
            for ii in range(total_samples):
                sample_name = list_class_files[keys]['sample_name'][ii]
                # Skip calibration samples as they are processed separately
                if sample_name not in calibration.values():
                    # Load the radial integration data for the current sample and detector
                    prefix = 'radial_integ'
                    sufix = 'dat'
                    ScanNr = list_class_files[keys]['scan'][ii]
                    det = list_class_files[keys]['detx_m'][ii]
                    Frame = 0 # Assuming integration is done per scan/frame 0
                    path_integ = path_dir_an + '/det_' + str(det).replace('.','p') + '/integration/'
                    # Construct the file name using the utility function from integration module
                    file_name = integ.make_file_name(path_integ, prefix, sufix, sample_name, str(det).replace('.','p'), ScanNr, Frame)

                    try:
                        # Load q, Intensity (I), and error (e) from the integrated data file
                        q = np.genfromtxt(file_name, dtype = None, delimiter = ',', usecols = 0)
                        I = np.genfromtxt(file_name, dtype = None, delimiter = ',', usecols = 1)
                        e = np.genfromtxt(file_name, dtype = None, delimiter = ',', usecols = 2) # Load error
                    except Exception as e_load:
                        print(f"Warning: Could not load data from {file_name}. Error: {e_load}. Skipping this file.")
                        continue # Skip to the next file if loading fails

                    # Specific correction for a particular beamtime/detector
                    #if det == 1.6:
                    #    I = np.divide(I,2) # Example: Divide intensity by 2 for det 1.6m
                    #    e = np.divide(e,2) # Propagate error for the intensity division

                    # Ensure errors are non-negative before storing
                    e = np.abs(e)

                    # Aggregate data for each sample from different detector distances
                    if sample_name in merged_files:
                        # If sample already exists, append new data vertically
                        temp_I = merged_files[sample_name]['I']
                        merged_files[sample_name]['I'] =  np.vstack((temp_I, I))
                        temp_q = merged_files[sample_name]['q']
                        merged_files[sample_name]['q'] =  np.vstack((temp_q, q))
                        temp_e = merged_files[sample_name]['error']
                        merged_files[sample_name]['error'] =  np.vstack((temp_e, e)) # Include error
                    else:
                        # If sample is new, initialize its entry in the dictionary
                        merged_files[sample_name] = {}
                        merged_files[sample_name]['I'] = I
                        merged_files[sample_name]['q'] = q
                        merged_files[sample_name]['error'] = e

    # Plot the raw (unmerged) data from all detectors for each sample
    for keys in merged_files:
        plt.close('all') # Close all existing plots
        plt.ioff() # Turn off interactive plotting to prevent window pop-ups

        if merged_files[keys]['q'].ndim > 1: # Check if data from multiple detectors exists
            dd = merged_files[keys]['q'].shape[0]
            for ii in range(dd):
                q = merged_files[keys]['q'][ii, :]
                I = merged_files[keys]['I'][ii,:]
                e = merged_files[keys]['error'][ii,:]
                plt.errorbar(q, I, e, lw = 0.3, marker = 'o',  ms = 2) # Plot with error bars

            plt.xlabel(r'Scattering vector q [$\AA^{-1}$]')
            plt.xscale('log')
            plt.yscale('log')
            plt.ylabel(r'Intensity I [cm$^{-1}$]')
            plt.title('Sample: '+  keys)
            file_name = path_merged_fig + keys + '_all_det_dist' + '.jpeg'
            plt.savefig(file_name) # Save the plot
        else:
            print('_____________________________')
            print('Single detector distance measurement: ' + keys)
            print('_____________________________')
    return merged_files

# %% merging_data
def merging_data(path_dir_an, merged_files, skip_start, skip_end, interp_type, interp_points, smooth_window):
    """
    Merges scattering data from different detector distances for each sample,
    handles overlapping regions, applies scaling, and performs interpolation and smoothing.

    Args:
        path_dir_an (str): The path to the main analysis directory.
        merged_files (dict): Dictionary of raw intensity, q, and error for each sample
                             from different detector distances (output of `plot_all_data`).
        skip_start (dict): Dictionary specifying number of points to skip from the
                           beginning of each detector's data (key: detector index as string).
        skip_end (dict): Dictionary specifying number of points to skip from the
                         end of each detector's data (key: detector index as string).
        interp_type (str): Type of interpolation ('log' for log-spaced q, 'linear' for linear q).
        interp_points (int): Number of points for the interpolated data.
        smooth_window (int): Window size for smoothing the interpolated data.
    """
    # Create necessary subdirectories for merged data and figures if they don't exist
    path_merged = os.path.join(path_dir_an, 'merged/')
    path_merged_fig = os.path.join(path_merged, 'figures/')
    if not os.path.exists(path_merged_fig):
        os.mkdir(path_merged_fig)
    path_merged_txt = os.path.join(path_merged, 'data_txt/')

    for keys in merged_files: # Iterate through each sample
        # Initialize lists to accumulate all q, I, and error values for the current sample
        I_all = []
        q_all = []
        e_all = []
        plt.close('all') # Close previous plots
        plt.ioff() # Turn off interactive plotting

        if merged_files[keys]['q'].ndim > 1: # Process only if data from multiple detectors exists
            # Determine the order of merging based on the first q-value of each detector.
            # This ensures merging proceeds from low q to high q.
            first_q_values = [merged_files[keys]['q'][kk][0] for kk in range(merged_files[keys]['q'].shape[0])]
            idx_det = np.argsort(first_q_values) # Get indices that would sort the array

            scaling = 1.0 # Initialize scaling factor for data adjustment

            # Loop through the detector indices in the sorted order (from low q to high q)
            for ii in idx_det:
                q = merged_files[keys]['q'][ii, :]
                I = merged_files[keys]['I'][ii,:]
                e = merged_files[keys]['error'][ii,:] # Load error

                # Retrieve skip points for the current detector, defaulting to 0 if not specified
                skip_start_val = skip_start.get(str(ii), 0)
                skip_end_val = skip_end.get(str(ii), 0)

                # Apply skipping to the data
                q = q[skip_start_val:len(q)-skip_end_val]
                I = I[skip_start_val:len(I)-skip_end_val]
                e = e[skip_start_val:len(e)-skip_end_val] # Apply skipping to error as well

                # If it's not the first detector in the sorted order, apply scaling
                if ii != idx_det[0]: # Check against the first index after sorting
                    # Define overlap region: last part of previously added data (q_all)
                    # and first part of current detector data (q, I, e)
                    # We need to find the overlap and calculate the scaling factor.
                    # A robust method would involve finding a common q-range for overlap.
                    # For simplicity and adhering to original logic, we take a median ratio.
                    # This part of the logic can be complex and might need more sophisticated alignment.

                    # For the purpose of applying the scaling factor from the previous segment
                    # to the current segment before concatenation:
                    # The `scaling` variable in the original code appears to be calculated
                    # and then implicitly applied to `I` and `e` before the next concatenation.
                    # Let's adjust the structure to apply scaling to the current (I, e) before adding to (I_all, e_all).

                    # Calculate overlap region and scaling only if q_all is not empty
                    if len(q_all) > 0 and len(q) > 0:
                        # Find the intersection of q ranges (a simplified approach)
                        # This assumes overlap exists and data quality in overlap is good.
                        min_q_overlap = max(q_all[0], q[0])
                        max_q_overlap = min(q_all[-1], q[-1])

                        # Create interpolation functions for the previous data (q_all, I_all)
                        # and current data (q, I) within the overlap region.
                        # Using 'nearest' or 'linear' for simple interpolation.
                        if max_q_overlap > min_q_overlap:
                            # Define interpolation points within the overlap
                            interp_q_overlap = np.linspace(min_q_overlap, max_q_overlap, 50) # Use a fixed number of points
                            try:
                                interp_prev_I = interp1d(q_all, I_all, kind='linear', fill_value="extrapolate")(interp_q_overlap)
                                interp_curr_I = interp1d(q, I, kind='linear', fill_value="extrapolate")(interp_q_overlap)

                                # Calculate median ratio, avoiding division by zero
                                valid_overlap = (interp_curr_I > 0) & (interp_prev_I > 0)
                                if np.any(valid_overlap):
                                    scaling = np.median(interp_prev_I[valid_overlap] / interp_curr_I[valid_overlap])
                                else:
                                    scaling = 1.0 # No valid overlap points, no scaling
                                if np.isnan(scaling) or np.isinf(scaling):
                                    scaling = 1.0
                            except ValueError as e:
                                print(f"Warning: Could not interpolate for scaling in sample {keys}, detector {ii}. Error: {e}. Setting scaling to 1.0.")
                                scaling = 1.0
                        else:
                            scaling = 1.0 # No overlap, no scaling
                    else:
                        scaling = 1.0 # No previous data or current data, no scaling

                    # Apply scaling to the current intensity and its error
                    I = np.multiply(I, scaling)
                    e = np.multiply(e, scaling) # Propagate error by multiplying with scaling factor

                # Ensure errors are non-negative after scaling
                e = np.abs(e)

                # Concatenate the current (potentially scaled) data to the overall lists
                # Ensure array is not empty before concatenation
                if q.size > 0:
                    q_all = np.concatenate((q_all, q), axis=None)
                    I_all = np.concatenate((I_all, I), axis=None)
                    e_all = np.concatenate((e_all, e), axis=None) # Include error in concatenation
                else:
                    print(f"Warning: Data for detector {ii} of sample {keys} is empty after skipping points. Skipping concatenation.")
                    pass # Skip concatenation if the array is empty


        else: # Case for single detector distance measurement
            print('_____________________________')
            print('Single detector distance measurement: ' + keys)
            print('_____________________________')
            q_all = merged_files[keys]['q']
            I_all = merged_files[keys]['I']
            e_all = merged_files[keys]['error'] # Error for single detector data
            e_all = np.abs(e_all) # Ensure errors are non-negative for single detector data too

        # Sort all aggregated data by q-value
        if q_all.ndim > 0 and q_all.size > 0: # Ensure q_all is not empty before sorting
            idx = np.argsort(q_all)
            q_all = q_all[idx]
            I_all = I_all[idx]
            e_all = e_all[idx] # Sort errors consistently with q and I

            # Plot original (merged, un-interpolated) data
            plt.errorbar(q_all, I_all, e_all, lw = 1, marker = 'o',  ms = 10, color = 'black',
                         alpha = 0.05, label = f'merged, scale = {np.round(scaling, 4)}') # Plot error bars

            interpolation_pts = None
            linear_results_I = None
            linear_results_e = None # New: for interpolated errors

            if interp_type == 'log':
                # Interpolate on a log-scale for q
                min_pt = np.log10(np.min(q_all[q_all > 0])) # Ensure q is positive for log
                max_pt = np.log10(np.max(q_all[q_all > 0]))
                interpolation_pts = np.logspace(min_pt, max_pt, interp_points)

                # Interpolate Intensity
                linear_interp_I = interp1d(q_all, I_all, kind='linear', fill_value="extrapolate")
                linear_results_I = linear_interp_I(interpolation_pts)
                # Smooth the interpolated intensity
                linear_results_I = smooth(linear_results_I, smooth_window)

                # Interpolate Error
                # Using 'linear' interpolation for errors. Consider 'nearest' if errors are sparse.
                linear_interp_e = interp1d(q_all, e_all, kind='linear', fill_value="extrapolate")
                linear_results_e = linear_interp_e(interpolation_pts)
                # Ensure interpolated errors are non-negative
                linear_results_e = np.abs(linear_results_e)
                # Smooth the interpolated errors (optional, use with caution)
                linear_results_e = smooth(linear_results_e, smooth_window)


                plt.loglog(interpolation_pts, linear_results_I, lw = 0.3,
                           marker = 'o',  ms = 4, color = 'red', label = 'interpolated')
                # Plot interpolated errors (as error bars or as a shaded region if preferred)
                plt.errorbar(interpolation_pts, linear_results_I, linear_results_e,
                             lw=0.1, color='red', alpha=0.3, fmt='none', label='interpolated errors')


            elif interp_type == 'linear':
                # Interpolate on a linear scale for q
                interpolation_pts = np.linspace(np.min(q_all), np.max(q_all), interp_points)

                # Interpolate Intensity
                linear_interp_I = interp1d(q_all, I_all, kind='linear', fill_value="extrapolate")
                linear_results_I = linear_interp_I(interpolation_pts)
                # Smooth the interpolated intensity
                linear_results_I = smooth(linear_results_I, smooth_window)

                # Interpolate Error
                linear_interp_e = interp1d(q_all, e_all, kind='linear', fill_value="extrapolate")
                linear_results_e = linear_interp_e(interpolation_pts)
                # Ensure interpolated errors are non-negative
                linear_results_e = np.abs(linear_results_e)
                # Smooth the interpolated errors (optional, use with caution)
                linear_results_e = smooth(linear_results_e, smooth_window)


                plt.loglog(interpolation_pts, linear_results_I, lw = 0.3,
                           marker = 'o',  ms = 4, color = 'red', label = 'interpolated')
                # Plot interpolated errors
                plt.errorbar(interpolation_pts, linear_results_I, linear_results_e,
                             lw=0.1, color='red', alpha=0.3, fmt='none', label='interpolated errors')


            plt.xlabel(r'Scattering vector q [$\AA^{-1}$]')
            plt.xscale('log')
            plt.yscale('log')
            plt.ylabel(r'Intensity I [cm$^{-1}$]')
            plt.title('Sample: '+  keys)
            plt.legend()

            file_name_fig = path_merged_fig + keys + '_merged' + '.jpeg'
            plt.savefig(file_name_fig)

            # Save the merged (un-interpolated) data to a text file
            header_text = 'q (A-1), I (1/cm), error' # Updated header
            file_name_txt = path_merged_txt +  keys +'_merged' + '.dat'
            data_save = np.column_stack((q_all, I_all, e_all)) # Include error
            np.savetxt(file_name_txt, data_save, delimiter=',', header=header_text)

            # Save the interpolated data if interpolation was performed
            if interp_type == 'linear' or interp_type == 'log':
                if interpolation_pts is not None and linear_results_I is not None and linear_results_e is not None:
                    file_name_interp_txt = path_merged_txt + keys + '_interp'  '.dat'
                    # Save interpolated q, I, and e
                    data_save_interp = np.column_stack((interpolation_pts, linear_results_I, linear_results_e))
                    np.savetxt(file_name_interp_txt, data_save_interp, delimiter=',', header=header_text)
        else:
            print(f"Warning: No valid data points to process for sample: {keys}. Skipping merging and saving.")


# %% subtract incoherent
def subtract_incoherent(path_dir_an, initial_last_points_fit=50, constancy_threshold=0.05):
    """
    Subtracts an incoherent flat background from SAXS/WAXS data.

    This function identifies a constant background in the high-q region of the
    scattering curve and subtracts it from the original data. It assumes that
    at high-q, the scattering from structural features has decayed sufficiently,
    leaving primarily a flat incoherent contribution.

    Args:
        path_dir_an (str): Path to the analysis directory containing 'merged' and 'config.npy'.
        initial_last_points_fit (int): Number of points from the end of the data to consider
                                       for fitting the flat background.
        constancy_threshold (float): A threshold (e.g., as a percentage of the mean intensity)
                                     to determine if the tail is "constant" enough for fitting.
                                     Smaller values mean stricter constancy.
    """
    path_merged = os.path.join(path_dir_an, 'merged')
    path_merged_txt = os.path.join(path_merged, 'data_txt')
    path_merged_fig = os.path.join(path_merged, 'figures')

    # Create directories if they don't exist
    os.makedirs(path_merged_txt, exist_ok=True)
    os.makedirs(path_merged_fig, exist_ok=True)

    file_name_config = os.path.join(path_dir_an, 'config.npy')
    try:
        with open(file_name_config, 'rb') as handle:
            config = pickle.load(handle)
    except FileNotFoundError:
        print(f"Error: config.npy not found at {file_name_config}. Please ensure the file exists.")
        return
    except Exception as e:
        print(f"Error loading config.npy: {e}")
        return

    calibration = config.get('experiment', {}).get('calibration', {})
    if not calibration:
        print("Warning: 'calibration' not found in config.npy or is empty. This might affect file processing.")

    files_to_process = []
    # Collect all relevant .dat files (merged or interpolated) from the merged data directory
    for file in os.listdir(path_merged_txt):
        if file.endswith('.dat') and ('interp' in file or 'merged' in file): # Look for 'interp' or 'merged' in filename
            files_to_process.append(file)

    if not files_to_process:
        print(f"No '.dat' files (interpolated or merged) found in {path_merged_txt}. Exiting.")
        return

    # MODIFIED: Simplified fitting model to only a flat background
    def flat_background_model(q_val, incoherent_val):
        """
        Scattering model for high-q region: only a flat incoherent background.
        I(q) = incoherent_val
        """
        return np.full_like(q_val, incoherent_val) # Returns an array of incoherent_val matching q_val's shape

    # Process each identified file
    for file_short_name in files_to_process:
        plt.close('all') # Close previous plots to prevent them from piling up in memory

        # Extract base name for output files and titles
        base_name = file_short_name.replace('_merged.dat', '').replace('_interp.dat', '')

        # Skip calibration files if they are in the list
        if base_name in calibration.values():
            print(f"Skipping {file_short_name} as it's a calibration file.")
            continue

        file_path_data = os.path.join(path_merged_txt, file_short_name)
        print(f"\nProcessing: {file_path_data}")

        try:
            # Load intensity (I), q-vector (q), and error (e) from the data file
            q = np.genfromtxt(file_path_data, dtype=float, delimiter=',', usecols=0)
            I = np.genfromtxt(file_path_data, dtype=float, delimiter=',', usecols=1)
            e = np.genfromtxt(file_path_data, dtype=float, delimiter=',', usecols=2) # Load errors
        except Exception as e:
            print(f"Error reading data from {file_path_data}: {e}. Skipping this file.")
            continue

        # Basic data validation
        if len(I) == 0 or len(q) == 0 or len(I) != len(q) or len(I) != len(e):
            print(f"Warning: Data arrays are empty or mismatched for {file_short_name}. Skipping.")
            continue
        if np.any(np.isnan(I)) or np.any(np.isnan(q)) or np.any(np.isinf(I)) or np.any(np.isinf(q)):
            print(f"Warning: Data contains NaN or Inf values for {file_short_name}. Skipping.")
            continue
        # Filter out zero or negative intensities for log plot and fitting, as they are problematic for log scale.
        # Also, filter errors to correspond to valid data.
        positive_mask = I > 0
        if not np.any(positive_mask):
            print(f"Warning: All intensities are zero or negative for {file_short_name}. Cannot perform log plot/fit. Skipping.")
            continue
        q_pos = q[positive_mask]
        I_pos = I[positive_mask]
        e_pos = e[positive_mask] # Filter errors consistently

        # Ensure errors are non-negative, as they represent standard deviations
        e_pos = np.abs(e_pos)
        # Avoid zero errors for weighted fitting, replace with a very small value if any are zero
        e_pos[e_pos == 0] = 1e-12


        if len(I_pos) < initial_last_points_fit + 5: # Ensure enough points for initial analysis + fit
            print(f"Warning: Not enough data points ({len(I_pos)}) for initial analysis and fitting in {file_short_name}. Skipping.")
            continue

        # --- Dynamic Constancy Check and Adaptable Fitting Range ---
        # Analyze the tail of the scattering curve to determine a stable region for fitting.
        # This involves checking for constancy in intensity and a minimal slope.
        # We look for a region that is genuinely flat for the incoherent background.
        tail_start_idx = max(0, len(I_pos) - initial_last_points_fit * 2) # Consider a broader tail region for analysis
        q_tail_analysis = q_pos[tail_start_idx:]
        I_tail_analysis = I_pos[tail_start_idx:]

        if len(I_tail_analysis) < 5: # Need at least a few points to assess constancy reliably
            print(f"Warning: Tail analysis range too small for {file_short_name}. Cannot assess constancy. Skipping.")
            # MODIFIED: Removed 'continue' here. The fit will now always be attempted.
            # This allows the user to decide if the fit is acceptable even if the automated check warns.

        # Check standard deviation relative to mean for intensity constancy
        # A smaller value for `constancy_threshold` means stricter constancy.
        if np.mean(I_tail_analysis) > 0:
            relative_std = np.std(I_tail_analysis) / np.mean(I_tail_analysis)
        else:
            relative_std = np.inf # If mean is zero, it's not constant/positive in a meaningful way

        # Check the slope of a linear fit to the tail on a linear scale.
        # This is crucial for verifying if the region is truly 'flat'.
        slope_lin, intercept_lin, r_value_lin, p_value_lin, std_err_lin = linregress(q_tail_analysis, I_tail_analysis)

        # Criteria for a "constant" tail: low relative standard deviation AND a very flat slope
        is_constant_tail = (relative_std < constancy_threshold) and \
                           (abs(slope_lin) < (constancy_threshold * np.mean(I_tail_analysis) / (q_tail_analysis[-1] - q_tail_analysis[0] + 1e-9)))

        if not is_constant_tail:
            # MODIFIED: Changed this from skipping the file to just printing a warning.
            print(f"Warning: Tail of data for {file_short_name} is not sufficiently constant (relative std: {relative_std:.3f}, slope: {slope_lin:.3e}). Proceeding with fit, but manual review recommended.")
        else:
            print(f"Tail of data for {file_short_name} appears constant (relative std: {relative_std:.3f}). Proceeding with fit.")


        # If the tail is considered constant, define the fitting range for the model.
        # We use `initial_last_points_fit` points for the fitting.
        determined_fitting_range = min(initial_last_points_fit, len(I_pos)) # Ensure we don't exceed available points

        # Ensure we have a minimum number of points for a robust fit (only 1 parameter to fit: incoherent_val)
        min_points_for_fit = 1 # Need at least 1 point to fit a constant, but more for robust average
        if determined_fitting_range < min_points_for_fit:
            print(f"Error: Determined fitting range ({determined_fitting_range} points) is too small for a robust fit for {file_short_name}. Skipping.")
            continue # This is a critical error, so we skip the file.

        # Select the data points for fitting the model
        off_set = len(I_pos) - determined_fitting_range
        fitting_I = I_pos[off_set:]
        fitting_q = q_pos[off_set:]
        fitting_e = e_pos[off_set:] # Corresponding errors for fitting

        print(f"Using {len(fitting_I)} points for fitting (q_range: {fitting_q[0]:.3e} to {fitting_q[-1]:.3e}).")

        # --- Initial Guess and Bounds for curve_fit ---
        # Only fitting `incoherent_val` now.
        initial_incoherent = np.mean(fitting_I) # Simple average of the fitting range
        if initial_incoherent <= 0:
            initial_incoherent = 1e-6 # Ensure a small positive guess

        p0 = [initial_incoherent] # Initial guess for the single parameter

        # Define bounds for the single parameter (incoherent_val)
        lower_bounds = [0] # Incoherent background must be non-negative
        upper_bounds = [np.max(fitting_I) * 2] # Should not be arbitrarily large

        try:
            # Perform the non-linear least squares fit using `scipy.optimize.curve_fit`
            # `sigma=fitting_e` is crucial for weighted least squares, giving more importance to points with smaller errors.
            params, cv = scipy.optimize.curve_fit(
                flat_background_model, fitting_q, fitting_I,
                p0=p0,
                sigma=fitting_e, # Use errors for weighted fitting
                bounds=(lower_bounds, upper_bounds),
                max_nfev=1000 # Max function evaluations
            )
            incoherent_fit = params[0] # Unpack the single fitted parameter

            # Calculate R-squared to assess the goodness of the fit
            residuals = fitting_I - flat_background_model(fitting_q, incoherent_fit)
            # to decrease a bit the level of the incohenet background
            incoherent_fit = incoherent_fit*0.99
            ss_res = np.sum(residuals**2)
            ss_tot = np.sum((fitting_I - np.mean(fitting_I))**2)
            rSquared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 1.0
            rSquared = round(rSquared, 2)

            # --- Plotting the fitted background and subtracted data ---
            # Create a single figure for the log-log plot
            fig, ax1 = plt.subplots(1, 1, figsize=(10, 7)) # Only one subplot now

            # Main log-log plot (ax1)
            ax1.errorbar(q_pos, I_pos, e_pos, fmt='.', lw=0.3, color='blue', alpha=0.5, label="Original data")

            # Plot the fitted incoherent background (flat line) across the full q range
            q_full_range = q_pos
            ax1.loglog(q_full_range, np.ones(len(q_full_range)) * incoherent_fit, '-', color='red', linewidth=2,
                       label=f"Fitted Incoherent Background = {incoherent_fit:.4f} (R²={rSquared})")

            # Plot the specific fitting region for clarity
            ax1.plot(fitting_q, fitting_I, 'o', color='orange', mfc='none', markersize=6, label='Data used for fit')

            # Subtracted data - now plotted on the SAME log-log axes.
            # Only positive values will be shown due to log scale.
            subtracted_I = I_pos - incoherent_fit
            subtracted_e = e_pos # Error remains the same if incoherent is treated as a perfect constant subtraction

            # Mask for positive subtracted data for log plotting
            valid_subtracted_mask = subtracted_I > 0
            ax1.errorbar(q_pos[valid_subtracted_mask], subtracted_I[valid_subtracted_mask],
                         subtracted_e[valid_subtracted_mask], fmt='o', ms=3, lw=0.5, color='black', label='Subtracted data (positive only)')


            ax1.set_title(f"Flat Background Subtraction for {base_name}")
            ax1.set_xlabel(r'Scattering vector q [$\AA^{-1}$]') # X-label only on main plot
            ax1.set_ylabel(r'Intensity I [cm$^{-1}$]')
            ax1.set_xscale('log')
            ax1.set_yscale('log')
            ax1.legend(loc='best') # Place legend automatically
            ax1.grid(True, which="both", ls="-", alpha=0.2)

            plt.tight_layout() # Adjust layout
            file_name_fig = os.path.join(path_merged_fig, f"{base_name}_flat_background_subtraction.jpeg")
            plt.savefig(file_name_fig, bbox_inches='tight', dpi=150)
            plt.close(fig) # Close the figure to free memory

            # Save the processed data (q, I_subtracted, error)
            header_text = 'q (A-1), I_subtracted (1/cm), error' # Updated header
            file_name_data_out = os.path.join(path_merged_txt, f"{base_name}_subtracted.dat")

            np.savetxt(file_name_data_out, np.column_stack((q_pos, subtracted_I, subtracted_e)),
                       delimiter=',', header=header_text, comments='# ')

        except (RuntimeError, ValueError) as e:
            print(f"Error during curve fitting or plotting for {file_short_name}: {e}. Skipping this file.")
            # Save original data if fit failed to ensure data is not lost or corrupted
            header_text = 'q (A-1), I (1/cm), error - Fit failed, data not subtracted'
            file_name_data_out = os.path.join(path_merged_txt, f"{base_name}_original_fit_failed.dat")
            np.savetxt(file_name_data_out, np.column_stack((q, I, e)), # Save original I and e
                       delimiter=',', header=header_text, comments='# ')
            continue
        except Exception as e:
            print(f"An unexpected error occurred for {file_short_name}: {e}. Skipping this file.")
            continue

    print("\nFlat background subtraction process completed.")
